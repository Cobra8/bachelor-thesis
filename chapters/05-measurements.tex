% !TEX root = ../main.tex

\chapter{Measurements}
\label{ch:measurements}
This work started from the existing SOSD benchmark going into the idea of implementing a learned index structure over the network in P4 and so far coming to a very theoretical result. The goal of this chapter is to try to get at least closer to understanding how much the ideas presented in the previous chapters could potentially benefit a concrete setup and implementation in the real world. As already quite extensively stated in section \ref{sect:rmionbmv2:evaluation} the P4 implementation as is cannot be tested on real world hardware. With this in mind the first section of this chapter will explain the choosen approach to still try to measure something useful in order to evaluate the potential impact. Further section \ref{sect:measurements:results} will present the measured results on the test university machine and finally the last section of this chapter will once again try to put the measured results into relation and give an evaluation.

\section{Method}
As previously stated the following method was choosen since not being able to actually run the generated P4 source code on some concrete real world hardware due to multiple reasons also already discussed previously. In order to come up with some alternative we started with the idea that instead of actually measuring lookup time on concrete network hardware we could instead measure pure lookup time in an existing implementation and go on from there taking what we measured as maximally possible speedup. This immediately leads to the already established separation between pure lookup operation time and time spent for the last mile search. For this having a working benchmark implemented in C++ that can precisely measure the time it takes for a specific amount of lookups including last mile search on a dataset comes in handy. In order to measure hypothetical maximal time gain it is enough to separate the last mile search and measure only the pure lookup time. The final piece to the puzzle now is that we have to assume that our network switches are able to process packets at a higher or at least a similar rate than a processor can handle pure lookup operations. This initially seemed like a bold claim to me but when taking into account that the proposed RMI implementation in P4 can easily scale horizontally with the amount of network hardware available, which is often available anyways, this quickly becomes realistic. Finally if we wanted to look at a closed system and effectively evaluate which method is faster, we would have to consider round trip time of packets. Although since not even being able to reliably test any of the implementation on actual hardware, this major concern for a real world setup is neglected in the scope of this work in the sense that we assume an application where packets need to travel over the network and with that over an RMI capable switch anyways and with that travel time spent would not really be lost, but instead used more efficiently.

\section{Results}
\label{sect:measurements:results}
In order to perform these measures we adapted the existing SOSD benchmark to measure not only lookup time and last mile search times at the same time but instead differenciate between the two steps and measuring only one at a time. The initial result though, when measuring only pure lookup operation time, is very disappointing. As shown in figure \ref{fig:books_200M_uint32-no-cc-mf} only a very small amount of time is spent on performing the pure RMI lookup operations.

\begin{figure}[!h]
  \centering
  \includegraphics[width=1\textwidth]{measurements/books_200M_uint32-no-cc-mf}
  \caption[Lookup and last mile search time measures \emph{without using} cold caches and memory fencing]{
    Running the SOSD benchmark on the \emph{books\_200M\_uint32} dataset \emph{without using} cold caches or memory fencing, differentiating between pure lookup time and last mile search time.
  }
  \label{fig:books_200M_uint32-no-cc-mf}
\end{figure}

There are reasons for the way these measurements turned out which were already pointed out in \cite{sosd-neurips} in chapter 4.4. Namely these reasons are that lookups in a tight loop can greatly benefit from low level CPU optimization techniques like operator reordering or caching. The same applies for the above measurement in an even more intense way, since only measuring performance of essentially lots of tightly repeated FMA instructions which the processor will optimize into a more optimal instruction order and therefore exaggerate the measured performance. The same holds true for caching, in the sense that some of the requested data will already be loaded in some cache level and therefore access time is greatly reduced. Luckily though the authors of \cite{sosd-neurips} suggest and also already implemented a way to mitigate these usually very desired CPU optimizations in the SOSD benchmark. The two proposed techniques are simple and aim at starting from a constant CPU state before every lookup calculation. The first technique called cold caching mitigates cache side effects by filling the L3 CPU cache with a constant randomly generated set of numbers before each lookup. The second technique called memory fencing aims at mitigating instruction reordering by introducing memory fences before each lookup using the appropriate CPU instruction. With these techniques now in place running SOSD takes a lot longer and RMI performance drastically decreases but with the advantage that now more reliable meaures can be taken. The results from running the SOSD benchmark on the same dataset as previously but now using cold caches and memory fencing are shown in figure \ref{fig:books_200M_uint32}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{measurements/books_200M_uint32}
  \caption[Lookup and last mile search time measures \emph{using} cold caches and memory fencing]{
    Running the SOSD benchmark on the \emph{books\_200M\_uint32} dataset \emph{using} cold caches and memory fencing, differentiating between pure lookup time and last mile search time.
  }
  \label{fig:books_200M_uint32}
\end{figure}

Further very similar observations can be made for all remaining 64-bit datasets provided by the SOSD benchmark shown in the appendix in section \ref{sect:appendix:measurements}. Finally the observation which leads to probably the best approximation of how long the pure RMI lookup operations actually take without cold caches and memory fencing, is when taking a normal measure (the red line in figure \ref{fig:books_200M_uint32-no-cc-mf}) and subtracting the last mile search time from this measure in order to guess the pure lookup operation time. This makes sense under the assumption that the last mile search code is less affected by operation reordering. This is then visualized in figure \ref{fig:books_200M_uint32-no-lookup}. As for previous measurements, similar graphs can be plotted for the remaining 64-bit datasets provided by the SOSD benchmark shown in the appendix in section \ref{sect:appendix:measurements-no-lookup}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{measurements/books_200M_uint32-no-lookup}
  \caption[Last mile search time measures and lookup time approximation \emph{without using} cold caches and memory fencing]{
    Running the SOSD benchmark on the \emph{books\_200M\_uint32} dataset \emph{without using} cold caches and memory fencing, trying to approximate pure lookup time by subtracting last mile search time from the totally measured time.
  }
  \label{fig:books_200M_uint32-no-lookup}
\end{figure}

\section{Evaluation}
After having described our measurement method in the first section and showing visualized results in the previous section, this section focuses on giving potential take-aways from what we measured. A first thing to note is that when looking at the figures in the previous section, one can observe that the height of the blue bar (representing the last mile search time) increases with increasing pareto variant. This is expected since the SOSD benchmark trains its RMIs with decreasing index size limit relative to increasing pareto variant, meaning that with decreasing index size the last mile search bound becomes larger and therefore time spent to find a key in said larger bound increases. In the previous chapter we intensively discussed the difference between measuring times while using techniques like cold caching or memory fencing. As \cite{sosd-neurips} also already pointed out, the intensity at which RMI benefits from low level CPU optimizations heavily depends on many factors such as the shape of the data itself and the actual optimization capability of the processor doing the calculations. This forcibly means that any measure becomes very heavily application dependant and therefore the best advice is still to test different (learned) index algorithms individually in a concrete application. The same holds for the idea proposed in this work. Still though, a regular CPU can highly improve RMIs performance in practice while switches do not currently support any sort of CPU optimization techniques at this level. This goes back to the fact that switches are very good and very fast at simple things but not more. Concretely this means that it makes most sense to compare and assume at most numbers measured in figure \ref{fig:books_200M_uint32} without any CPU optimizations at play since this is what will happen on the switch, but when opting for a comparision under ideal conditions, refer to figure \ref{fig:books_200M_uint32-no-lookup}.\\

The point of this chapter is to show that in the future under ideal conditions outsourcing a learned index structure such as RMI to the network can lead to a speedup. Still though the major drawback that this work makes apparant is that there is a long way to go. There are lots of requirements currently missing in real world hardware and lots of other assumptions and simplifications that have been taken. Namely these consist of assuming that a server that wants to perform a RMI lookup needs to send a packet to a distant P4 capable network switch anyways, that the proposed P4 implementation can be reduced to less CPU stages or that newer switches have more CPU stages and similarly that newer switches can handle 64-bit width or wider basic bit operations. Finally assuming all these difficult conditions are given, a currently modern Intel® Tofino™ 3 switch can handle under ideal conditions up to 10 billion packets per second \cite{tofino3-brief}. From here on though lots of considerations become quite a bit unscientific and speculative and therefore this chapter should be taken as such. Importantly though, what matters for the take-away in my opinion are the magnitudes, not the exact numbers since those are quite arbitratily measured and put together, but still the general idea of a potential speedup remains. In any case, our university machine's CPU takes around 80 - 400ns for a full RMI lookup (including pure lookup operations and last mile search) depending on the dataset without cold caches and memory fences (Refering to figure \ref{fig:sosd_lookups}). This means that our machine can handle between 2.5 - 12.5 million full RMI lookups per second. When we now refer to the graphs in figure \ref{fig:books_200M_uint32-no-lookup} and section \ref{sect:appendix:measurements-no-lookup} we find that to do the pure lookup calculations it takes pretty constanntly roughly around 100ns. We finally find that our university machine which would only have to deal with last mile searches and could outsource the pure lookup calculations to the switch could handle between 3 - 20 million last mile searches per second and takes around 50 - 300ns for a single last mile search. All in all we find that by outsourcing the RMI calculations to the switch we can constantly gain around 100ns per lookup independently of the RMI's index size and with that last mile search bound size, which means that without the RMI calculations a server can constantly handle about 10 million more lookups per second. Further, if only a single Intel® Tofino™ 3 switch can provide pure lookup operations at a rate of around 10 billion packets per second this switch would easily outperform multiple magnitudes of servers working on last mile searches. This means that in a closed system not the amount of network switches determine the amount of speed gain when outsourcing RMI to the network but instead the amount of last mile search workers do. Additionally the constantly less time not spent on the server for calculating pure lookups would be mostly free, especially when having an application in mind where a lookup has to be sent over the network anyways.
